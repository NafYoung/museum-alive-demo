import streamlit as st
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Page Config
st.set_page_config(page_title="Museum Alive", page_icon="ğŸ›ï¸")

# Title and Description
st.title("ğŸ›ï¸ Museum Alive: Let Artifacts Speak")
st.write("Upload a photo of an artifact, and AI will bring it to life.")

# Sidebar for Settings
with st.sidebar:
    st.header("Settings")
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key or "your-key-here" in api_key:
        st.warning("âš ï¸ Please set your DEEPSEEK_API_KEY in the .env file.")
    else:
        st.success("âœ… API Key Loaded")

import asyncio
import edge_tts
from openai import OpenAI

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

# Initialize DeepSeek Client
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# Initialize Vision Model (Moondream) - Cached to avoid reloading
@st.cache_resource
def load_vision_model():
    model_id = "vikhyatk/moondream2"
    revision = "2024-04-02"
    model = AutoModelForCausalLM.from_pretrained(
        model_id, trust_remote_code=True, revision=revision
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
    return model, tokenizer

vision_model, vision_tokenizer = load_vision_model()

async def generate_audio(text, output_file="output.mp3"):
    """Generate audio using Edge-TTS (Free)"""
    communicate = edge_tts.Communicate(text, "zh-CN-YunxiNeural")
    await communicate.save(output_file)

def analyze_image(image):
    """Use Moondream to describe the image"""
    enc_image = vision_model.encode_image(image)
    # Prompt Moondream to describe the artifact
    description = vision_model.answer_question(enc_image, "Describe this artifact in detail.", vision_tokenizer)
    return description

def get_artifact_story(artifact_description):
    """Ask DeepSeek to roleplay based on visual description"""
    prompt = f"""
    æˆ‘ç»™ä½ çœ‹äº†ä¸€å¼ æ–‡ç‰©çš„å›¾ç‰‡ï¼Œå®ƒçš„ç‰¹å¾æ˜¯ï¼š{artifact_description}ã€‚
    
    è¯·ä½ æ ¹æ®è¿™ä¸ªæè¿°ï¼ŒçŒœçŒœä½ å¯èƒ½æ˜¯è°ï¼ˆå¦‚æœç‰¹å¾å¾ˆæ˜æ˜¾ï¼‰ï¼Œæˆ–è€…å°±ä½œä¸ºä¸€ä¸ªç¥ç§˜çš„å¤ç‰©ã€‚
    
    è¯·ç”¨ç¬¬ä¸€äººç§°ï¼ˆâ€œæˆ‘â€ï¼‰åšä¸€ä¸ªè‡ªæˆ‘ä»‹ç»ã€‚
    
    è¦æ±‚ï¼š
    1. æ—¢ç„¶æ˜¯â€œè®©æ–‡ç‰©è¯´è¯â€ï¼Œè¯­æ°”è¦ç¬¦åˆä½ çš„èº«ä»½ã€‚
    2. ä¸è¦åªè®²æ¯ç‡¥çš„æ•°æ®ï¼Œè¦è®²ä½ çš„æ„Ÿå—ã€‚
    3. ç¯‡å¹…æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ã€‚
    4. å¼€å¤´è¦å¸å¼•äººã€‚
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåšç‰©é¦†é‡Œçš„æ–‡ç‰©ï¼Œå¯Œæœ‰æ€§æ ¼å’Œæƒ…æ„Ÿã€‚"},
                {"role": "user", "content": prompt},
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"å“å‘€ï¼Œæˆ‘çœ‹ä¸æ¸…è‡ªå·±... ({str(e)})"

# Main Content
uploaded_file = st.file_uploader("ğŸ“¸ ç»™ä»–æ‹å¼ ç…§ (æˆ–ä¸Šä¼ å›¾ç‰‡)", type=["jpg", "png", "jpeg"])

if uploaded_file is not None:
    st.image(uploaded_file, caption="å·²ä¸Šä¼ æ–‡ç‰©", use_container_width=True)
    
    if st.button("è®©å®ƒè¯´è¯ ğŸ—£ï¸"):
        if not api_key:
            st.error("è¯·å…ˆåœ¨ .env æ–‡ä»¶ä¸­é…ç½® DEEPSEEK_API_KEY")
        else:
            with st.spinner("æ­£åœ¨è§‚å¯Ÿè¿™ä¸ªæ–‡ç‰© (AI è¯†å›¾ä¸­)..."):
                # 0. Load Image
                image = Image.open(uploaded_file)
                
                # 1. Vision Analysis
                description = analyze_image(image)
                st.info(f"ğŸ‘€ æˆ‘çœ‹åˆ°çš„ï¼š{description}")
                
                with st.spinner("æ­£åœ¨å”¤é†’æ²‰ç¡çš„çµé­‚..."):
                    # 2. Generate Story
                    story = get_artifact_story(description)
                    st.markdown(f"### ğŸ“œ æ–‡ç‰©çš„è‡ªè¿°")
                    st.write(story)
                    
                    # 3. Generate Audio
                    output_file = "artifact_voice.mp3"
                    asyncio.run(generate_audio(story, output_file))
                    
                    # 4. Play Audio
                    st.audio(output_file)
                    st.success("ğŸ‰ å”¤é†’æˆåŠŸï¼")
